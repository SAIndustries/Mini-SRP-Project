{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tM4l7j5Vnpj"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \" \"\n",
        "\n",
        "from apiclient.discovery import build\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "ID = \" \"\n",
        "\n",
        "box = [['Name', 'Comment', 'Time', 'Likes', 'Reply Count']]\n",
        "\n",
        "def scrape_comments_with_replies():\n",
        "  data = youtube.commentThreads().list(part='snippet', videoId-ID, maxResults='100', textFormat=\"plainText\").execute()\n",
        "\n",
        "  for i in data[\"items\"]:\n",
        "\n",
        "    name = i[\"snipper\"]['topLevelComment'][\"snippet\"][\"authorDisplayName\"]\n",
        "    comment = i[\"snippet\"]['topLevelComment'][\"snippet\"][\"textDisplay\"]\n",
        "    published_at = i[\"snippet\"]['topLevelComment'][\"snippet\"]['publishedAt']\n",
        "    likes = i[\"snippet\"]['topLevelComment'][\"snippet\"]['likeCount']\n",
        "    replies = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "    box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "    totalReplyCount = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "    if totalReplyCount > 0:\n",
        "      \n",
        "      parent = i[\"snippet\"]['topLevelComment'][\"id\"]\n",
        "\n",
        "      data2 = youtube.comments().list(part='snippet', maxResults='100', parentId=parent, textFormat=\"plainText\").execute()\n",
        "\n",
        "      for i in data2[\"items\"]:\n",
        "        name = i[\"snippet\"][\"authorDisplayName\"]\n",
        "        comment = i[\"snippet\"][\"textDisplay\"]\n",
        "        published_at = i[\"snippet\"]['publishedAt']\n",
        "        likes = i[\"snippet\"]['likeCount']\n",
        "        replies = \" \"\n",
        "\n",
        "        box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "  while (\"nextPageToken\" in data):\n",
        "\n",
        "    data = youtube.commentThreads().list(part='snippet', videoId=ID, pageToken=data[\"nextPageToken\"], maxResults='100', textFormat=\"plainText\").execute()\n",
        "\n",
        "    for i in data[\"items\"]:\n",
        "      name = i[\"snippet\"]['topLevelComment'][\"snippet\"][\"authorDisplayName\"]\n",
        "      comment = i[\"snippet\"]['topLevelComment'][\"snippet\"][\"textDisplay\"]\n",
        "      published_at = i[\"snippet\"]['topLevelComment'][\"snippet\"]['publishedAt']\n",
        "      likes = i[\"snippet\"]['topLevelComment'][\"snippet\"]['likeCount']\n",
        "      replies = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "      box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "      totalReplyCount = i[\"snippet\"]['totalReplyCount']\n",
        "\n",
        "      if totalReplyCount > 0:\n",
        "\n",
        "        parent = i[\"snippet\"]['topLevelComment'][\"id\"]\n",
        "\n",
        "        data2 = youtube.comments().list(part='snippet', maxResults='100', parentId=parent, textFormat=\"plainText\").execute()\n",
        "\n",
        "        for i in data2[\"items\"]:\n",
        "          name = i[\"snippet\"][\"authorDisplayName\"]\n",
        "          comment = i[\"snippet\"][\"textDisplay\"]\n",
        "          published_at = i[\"snippet\"]['publishedAt']\n",
        "          likes = i[\"snippet\"]['publishedAt']\n",
        "          replies = ' '\n",
        "\n",
        "          box.append([name, comment, published_at, likes, replies])\n",
        "\n",
        "  df = pd.DataFrame({'Name': [i[0] for i in box], 'Comment': [i[1] for i in box], 'Time': [i[2] for i in box], 'Likes': [i[3] for i in box], 'Reply Count': [i[4] for i in box]})\n",
        "\n",
        "  df.to_csv('youtube-comments.csv', index=False, header=False)\n",
        "\n",
        "  return \"Successful! Check the CSV File you have created\""
      ],
      "metadata": {
        "id": "T0KDndlNYA3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_comments_with_replies()"
      ],
      "metadata": {
        "id": "PbKsm78id3QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.displau.max_row = 10\n",
        "\n",
        "df = pd.read_csv('youtube-comments.csv', index_col=0)\n",
        "df"
      ],
      "metadata": {
        "id": "frPiBD3Ld7YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "GQSM4_uXeKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comment'].nunique()"
      ],
      "metadata": {
        "id": "rYw-X3NfeMTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "A89Yzv_xeQTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "stop_words[:10]"
      ],
      "metadata": {
        "id": "oMNpd_eNeczv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hashtags(text):\n",
        "  hashtags = re.findall(r'\\#\\w+',text.lower())\n",
        "  return hashtags\n",
        "def get_mentions(text):\n",
        "  mentions = re.findall(r'\\@\\w+',text.lower())\n",
        "  return mentions\n",
        "\n",
        "def remove_content(text):\n",
        "  text = re.sub(r\"http\\S+\",\"\", text)\n",
        "  text = re.sub(r'\\S+\\.com\\S+','', text)\n",
        "  text = re.sub(r'\\@\\w+', '', text)\n",
        "  text = re.sub(r'\\#\\w+','', text)\n",
        "  return text\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",tweet.lower()).split())\n",
        "\n",
        "def process_text(text, stem=False):\n",
        "  text=remove_content(text)\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  text = re.sub('[^A-Za-z', ' ', text.lower())\n",
        "  text = re.sub(r'@[A-Za-Z0-9]+', ' ', str(text))\n",
        "  text = re.sub(r'#', ' ', str(text))\n",
        "  text = re.sub(r'RT[\\s]+', '', str(text))\n",
        "  text = re.sub(r'https?\\/\\/S+', ' ', str(text))\n",
        "  text = re.sub(r'http\\S+', '', str(text))\n",
        "  text = re.sub(r'www\\S+', '', str(text))\n",
        "  text = re.sub(r'pic+', ' ', str(text))\n",
        "  text = re.sub(r'com', '', str(text))\n",
        "  text = re.sub(r\"\\bamp\\b\", ' ', text.lower())\n",
        "  text = re.sub(r\"\\bco\\b\", ' ', text.lower())\n",
        "  tokenized_text = word_tokenize(text)\n",
        "\n",
        "  clean_text=[\n",
        "      word for word in tokenized_text\n",
        "      if (word not in stop_words and len(word)>1)\n",
        "  ]\n",
        "  if stem:\n",
        "    clean_text=[stemmer.stem(word) for word in clean_text]\n",
        "    clean_text = [lemmatizer.lemmatize(word) for word in clean_text]\n",
        "    return ' '.join(clean_text)\n",
        "\n",
        "def removeDupWithoutOrder(string):\n",
        "  words = string.lower().split()\n",
        "  return \" \".join(sorted(set(words), key=words.index)).replace('OR', ' ').replace(' ',' ')\n",
        "\n",
        "def remove_search(text, search_terms):\n",
        "  query = text.lower()\n",
        "  querywords = query.split()\n",
        "  resultwords = [word for word in querywords if word.lower() not in search_terms]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "def plot_topn(sentences, ngram_range=(1,3), top=20, firstword=' '):\n",
        "  c=CountVectorizer(ngram_range=ngram_range)\n",
        "  X=c.fit_transform(sentences)\n",
        "  words=pd.DataFrame(X.sum(axis=0),columns=c.get_featur_names()).T.sort_values(0,ascending=False).reset_index()\n",
        "  res = words[words['index'].apply(lambda x: firstword in x)].head(top)\n",
        "  pl=px.bar(res, x='index',y=0)\n",
        "  pl.update_layout(yaxis_title='count',xaxis_title='Phrases')\n",
        "  pl.show('png')"
      ],
      "metadata": {
        "id": "v7QhWZZHekpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "re.compile('<title>(.*)</title>')"
      ],
      "metadata": {
        "id": "T_LBJGN0yhRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Comment']=df['Comment'].apply(lambda x: remove_content(x))"
      ],
      "metadata": {
        "id": "k_T724tFyq3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_comments']=df['Comment'].apply(lambda x: process_tweet(x))"
      ],
      "metadata": {
        "id": "h6kBPfvFy0U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "FVLsQ1yPy9iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud,ImageColorGenerator\n",
        "from PIL import Image\n",
        "import urllib\n",
        "import requests\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rY0A1PcLy-E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_words = ' '\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "for val in df.cleaned_comments:\n",
        "  val = str(val)\n",
        "\n",
        "  tokens = val.split()\n",
        "\n",
        "  for i in range(len(tokens)):\n",
        "    tokens[i] = tokens[i].lower()\n",
        "    comment_words += \" \",join(tokens)+\" \"\n",
        "\n",
        "wordcloud = WordCloud(width = 1000, height = 800, background_color='white', colormap='Set2', collocations=False, stopwords = stopwords, normalize_plurals=False, min_font_size=12).generate(comment_words)\n",
        "\n",
        "plt.figure(figsize = (10,10), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FR6HTx-fzRQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "65SpLk7k0TJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSubjectivity(text):\n",
        "  return TextBlob(ste(text)).sentiment.subjectivity\n",
        "\n",
        "def getPolarity(text):\n",
        "  return TextBlob(str(text)).sentiment.polarity"
      ],
      "metadata": {
        "id": "_bbMmR9k0g9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp.dropna(subset=['cleaned_comments'], inplace = True)\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "P3obdPoW0uWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Subjectivity'] = df['cleaned_comments'].apply(getSubjectivity)\n",
        "df['Polarity'] = df['cleaned_comments'].apply(getPolarity)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rJQbFrDs04_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Polarity_Analysis(score):\n",
        "  if score < 0:\n",
        "    return 'Negative'\n",
        "  elif score == 0:\n",
        "    return 'Neutral'\n",
        "  else:\n",
        "    return 'Positive'\n",
        "\n",
        "  def get_Subjectivity_Analysis(score):\n",
        "    if score > 0:\n",
        "      return 'Opinion'\n",
        "    else:\n",
        "      return 'Fact'\n",
        "\n",
        "df['Analysis_Polarity'] = df['Polarity'].apply(get_Polarity_Analysis)\n",
        "\n",
        "df['Analysis_Subjectivity'] = df['Subjectivity'].apply(get_Subjectivity_Analysis)"
      ],
      "metadata": {
        "id": "XY0SBeUo1HOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=2)"
      ],
      "metadata": {
        "id": "KwQywch-122y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "plt.scatter(df['Polarity'], df['Subjectivity'], c=df['Polarity'], s=100, cmap= 'RdY1Gn')\n",
        "\n",
        "plt.xlim(-1.1, 1.1)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dq74RrMb2A11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.title('Polarity Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "df['Analysis_Polarity'].value_counts().plot(kind = 'bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FKwrj2kV2fVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize(7,5))\n",
        "\n",
        "plt.title('Subjectivity Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "df['Analysis_Subjectivity'].value_counts().plot(kind = 'bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5qzvZdJs2yXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGv7vkJe3JsH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}